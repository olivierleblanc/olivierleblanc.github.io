var store = [{
        "title": "Getting helped by ChatGPT",
        "excerpt":"  Introduction    ","categories": [],
        "tags": [],
        "url": "/Getting_helped_by_ChatGPT/",
        "teaser": null
      },{
        "title": "Some tips for scientists",
        "excerpt":"  Motivation   One benefit I got throughout my last years at University is a series of small tips easing the daily work of a scientific researcher. Let me share some of it here with you:   LaTeX everywhere   The worldwide convention to share scientific content is , it allows writing proper math equations and editing a text document in a codable way. Compared to Microsoft Word, it reduces the risks to move the whole document content when adding some object in the middle, at cost of getting familiar with the main commands.   Sometimes, it may be useful to write LaTeX text on other applications.   [TO BE CONTINUED]  ","categories": [],
        "tags": [],
        "url": "/Tips/",
        "teaser": null
      },{
        "title": "Low-rank circulant matrices",
        "excerpt":"         Theorem.  A circulant matrix \\(\\bs C \\in \\Cbb^{N\\times N}\\) which has been generated using the DFT \\(\\bs u = \\bs{Fv} \\in \\Cbb^N\\) (with \\(\\bs F\\) the DFT matrix) of a \\(K\\)-sparse vector \\(\\bs v \\in \\Cbb^N\\) is rank-\\(K\\).    Proof.  If \\(\\bs v\\) is \\(K\\)-sparse (\\(\\norm{}{\\bs v}{0} \\le K\\)), it can be written as   \\[\\begin{equation} \\tag{1} \\label{eq:sparse}     \\bs v = \\sum_{q=0}^{K-1} \\rho_q \\bs e_{p_q}. \\end{equation}\\]  Moreover, let \\(T(\\bs u)\\) be the operator that turns a vector \\(\\bs u \\in \\Cbb^N\\) into a circulant matrix, that is, in modulo arithmetic (\\(u_{k-l} = u_{\\modulo{k-l}{N}}\\))   \\[\\begin{equation}     (T(\\bs u))_{k,l} = u_{k-l} \\end{equation}\\]  We can write \\(\\bs C = T(\\bs u) = T(\\bs{Fv})\\). We also have \\(u_a = \\sum_{b=0}^{N-1} F_{a,b} v_b\\). The circulant matrix at index \\(jk\\) can thus write   \\[\\begin{align} \\tag{2} \\label{eq:dev} \\begin{split}     C_{jk} &amp;= u_{j-k} = \\sum_{l=0}^{N-1} F_{j-k,l}~ v_l      \\underset{\\eqref{eq:sparse}}{=} \\sum_{l=0}^{N-1} F_{j-k,l}      \\sum_{q=0}^{K-1} \\rho_q \\delta_{l,p_q} \\\\      &amp;= \\sum_{q=0}^{K-1} \\rho_q F_{j-k,p_q }      \\underbrace{\\sum_{l=0}^{N-1} \\delta_{l,p_q}}_{=1} \\\\     &amp;= \\sum_{q=0}^{K-1} \\rho_q F_{j,p_q} F_{k,p_q}^* \\end{split} \\end{align}\\]  where the last line in Eq.~\\eqref{eq:dev} is obtained thanks to the  definition of the coefficients of the DFT matrix  \\(F_{jk} = e^{\\frac{-\\im 2\\pi jk}{N}}\\). This is the critical property which  allows this proof. Finally, the circulant matrix writes as   \\[\\begin{equation} \\tag{3} \\label{eq:not1}     \\bs C = \\sum_{q=0}^{K-1} \\rho_q \\bs f[p_q] \\bs f[p_q]^* \\end{equation}\\]  with \\(\\bs f [n] := \\bs F_{:,n}\\), which makes it rank-\\(K\\).   The circulant matrix \\(\\bs C\\) in \\eqref{eq:not1} can also be equivalently represented as   \\[\\begin{equation} \\tag{4} \\label{eq:quasiSVD}     \\bs C = \\begin{bmatrix}         &amp; &amp; &amp; \\\\         &amp; &amp; &amp; \\\\         \\bs f [p_0] &amp; \\bs f [p_1] &amp; \\dots \\\\         &amp; &amp; &amp; \\\\         &amp; &amp; &amp;     \\end{bmatrix} \\begin{bmatrix}         \\rho_0 &amp; &amp; &amp; \\\\         &amp; \\rho_1 &amp; &amp; \\\\         &amp; &amp; \\rho_2 &amp; \\\\          &amp; &amp; &amp; \\ddots      \\end{bmatrix} \\begin{bmatrix}         &amp; &amp; \\bs f^*[p_0] &amp; &amp; \\\\         &amp; &amp; \\bs f^*[p_1] &amp; &amp; \\\\         &amp; &amp; \\vdots &amp; &amp; \\\\     \\end{bmatrix} = \\bs{FP} \\diag(\\bs \\rho) \\bs P^* \\bs F^*, \\end{equation}\\]  with \\(\\bs \\rho \\in \\Rbb^K\\) containing the \\(K\\) coefficients of the vector \\(\\bs v\\) which also shows \\(\\bs C\\) is rank-\\(K\\).   Note: With the conventional writing \\(\\bs v = \\sum_{l=0}^{N-1} v_l \\bs e_l\\), a derivation similar to \\eqref{eq:dev} would lead to \\(\\bs C = \\sum_{l=0}^{N-1} v_l \\bs f[l] \\bs f[l]^*\\).   Thus   \\[\\begin{equation} \\label{eq:decomp} \\bs C = \\bs F \\diag( \\bs v) \\bs F^*     \\end{equation}\\]  We see that \\eqref{eq:quasiSVD} is very close to the SVD defition:  \\(\\bs C := \\bs U \\bs{\\Sigma V}^*\\). The subtlety comes where the singular values in \\(\\bs \\Sigma\\) are (i) all positive and (ii) aranged in descending order.  (i) implies \\(\\bs C = \\bs F \\bs P |\\diag(\\bs \\rho)| \\bs P^* \\tilde{\\bs F}^*\\) where \\(\\tilde{\\bs F}^*\\) is the Fourier matrix with each column \\(i\\) multiplied by \\(+1\\) or \\(-1\\) depending on the sign of associated \\(\\rho_i\\).  (ii) implies the selection matrix \\(\\bs P\\) must also contain permutation to ensure \\(\\bs \\rho\\) is aranged in descending order.   To synthetize, we have   \\[\\begin{align} \\bs C &amp;= \\bs F \\diag (\\bs v) \\bs F^* \\\\ &amp;= \\bs F \\diag(|\\bs v|) \\bs S \\bs F^*\\\\ &amp;= \\bs F \\bs P^* \\diag(|\\tilde{\\bs v}|) \\bs{PSF}^* \\\\ &amp;= \\bs{U\\Sigma V}^*, \\end{align}\\]  with \\(\\bs S = \\diag(\\text{sign}(\\bs v))\\), \\(\\bs P\\) is a permutation matrix such that  \\(\\bs P |\\bs v| = |\\tilde{\\bs v}|\\) and \\(\\tilde{v}_k \\ge |\\tilde{v}_{k+1}|\\), \\(\\bs \\Sigma = \\diag(|\\tilde{\\bs v}|)\\), and the unitary matrices \\(\\bs U = \\bs F \\bs P^*\\), and  \\(\\bs V = \\bs F \\bs S \\bs P\\). By definition, this means that  \\(\\bs U \\bs \\Sigma \\bs V^*\\) is the SVD decomposition of \\(\\bs C\\).   In fact, \\(\\bs U \\bs \\Sigma \\bs V^* = \\bs U \\bs Z \\bs \\Sigma \\bs Z^* \\bs V^*\\) for any \\(\\bs Z = {\\rm diag}(\\bs z)\\) such that \\(\\bs Z \\bs Z^* = \\bs I\\)  (that is, \\(\\bs z\\) is such that \\(|z_k|=1\\)).  So we can choose \\(\\bs z\\) to make \\(\\bs U\\bs Z\\) and \\(\\bs V \\bs Z\\) real,  as in J. Romberg developments.   This proves a rank-\\(K\\) circulant matrix contains only \\(K\\) degrees of freedom, very less than a general rank-\\(K\\) matrix containing \\((2K+1)N\\) DOFs.   We also see a circulant matrix is hermitian if \\(\\bs v \\in \\Rbb^N\\) as \\(\\bs C^* = \\bs F \\diag(\\bs v)^* \\bs F^* = \\bs F \\diag(\\bs v) \\bs F^* = \\bs C \\Leftrightarrow \\bs v = \\bs v^*\\).  ","categories": [],
        "tags": [],
        "url": "/GD_lowrank_circulant_mat/",
        "teaser": null
      },{
        "title": "The ideal constant stepsize of gradient descent",
        "excerpt":"      In the aim to compute an approximate solution to the linear inverse problem \\(\\bs{Ax}=\\bs y\\) with \\(\\bs A \\in \\Rbb^{n\\times n}\\) of rank \\(r \\leq n\\), one is interested in evaluating the loss \\begin{equation}     \\cl L (\\bs x^{(k)}, \\bs y) = \\tinv 2 \\norm{}{\\bs A \\bs x^{(k)} -\\bs y}{2}^2 \\end{equation} when \\(\\bs x^{(k)}\\) is the \\(k\\)-th iterate of the \\emph{Gradient Descent} algorithm with zero initialization. It writes \\(\\bs x^{(k)} = \\sum_{i=0}^k (\\bs I-\\alpha \\bs A^* \\bs A)^i \\alpha \\bs A^* \\bs y\\), and writing the SVD decomposition \\(\\bs A := \\bs U \\bs \\Sigma \\bs V^*\\) with the properties \\(\\bs V^* \\bs V = \\bs I_r\\) and \\(\\bs U^* \\bs U = \\bs I_r\\), one gets   \\[\\begin{align}     \\bs A \\bs x^{(k)} &amp;= \\sum_{i=0}^k \\bs U \\bs \\Sigma \\bs V^* (\\bs I_n - \\alpha \\bs V \\bs\\Sigma^2 \\bs V^*) (\\bs I_n - \\alpha \\bs A^* \\bs A)^{i-1} \\alpha \\bs A^* \\bs y \\\\     &amp;= \\sum_{i=0}^k \\bs U \\bs \\Sigma (\\bs I_r - \\alpha \\bs\\Sigma^2) \\bs V^* (\\bs I_n - \\alpha \\bs A^* \\bs A)^{i-1} \\alpha \\bs A^* \\bs y \\\\     &amp;= \\sum_{i=0}^k \\bs U \\bs \\Sigma (\\bs I_r - \\alpha \\bs\\Sigma^2)^i \\bs V^* \\alpha \\bs A^* \\bs y \\\\     &amp;= \\bs U \\bs\\Sigma \\underbrace{\\sum_{i=0}^k (\\bs I_r - \\alpha \\bs\\Sigma^2)^i}_{(\\alpha \\bs\\Sigma^2)^{-1} (\\bs I_r - (\\bs I_r - \\alpha \\bs\\Sigma^2)^{k+1})} \\alpha \\bs\\Sigma \\bs U^* \\bs y \\\\     &amp;= \\bs U \\bs\\Sigma^{-1} (\\bs I_r - (\\bs I_r - \\alpha \\bs\\Sigma^2)^{k+1}) \\bs\\Sigma \\bs U^* \\bs y \\\\     &amp;= \\bs U (\\bs I_r - (\\bs I_r - \\alpha \\bs\\Sigma^2)^{k+1}) \\bs U^* \\bs y. \\end{align}\\]  Hence,   \\[\\begin{align} \\tag{1} \\label{eq:loss} \\begin{split}     \\cl L (\\bs x^{(k)}, \\bs y) &amp;= \\tinv 2 \\norm{\\bigg}{ \\underbrace{(\\bs U \\bs U^* - \\bs I_n) \\bs y}_{\\perp \\text{Im } \\bs U } - \\bs U \\diag \\bigg( 1-\\eta \\big( \\frac{\\sigma_i}{\\sigma_{\\rm max}} \\big)^2 \\bigg)^{k+1} \\bs U^* \\bs y }{2}^2 \\\\     &amp;= \\tinv 2 \\norm{\\bigg}{ (\\bs U \\bs U^* - \\bs I_n) \\bs y}{2}^2 + \\tinv 2 \\norm{\\bigg}{ \\bs U \\diag \\bigg( 1-\\eta \\big( \\frac{\\sigma_i}{\\sigma_{\\rm max}} \\big)^2 \\bigg)^{k+1} \\bs U^* \\bs y }{2}^2 \\\\     &amp;= \\tinv 2 \\norm{\\bigg}{ (\\bs U \\bs U^* - \\bs I_n) \\bs y}{2}^2 + \\tinv 2 \\norm{\\bigg}{ \\diag \\bigg( 1-\\eta \\big( \\frac{\\sigma_i}{\\sigma_{\\rm max}} \\big)^2 \\bigg)^{k+1} \\bs U^* \\bs y }{2}^2 \\\\     &amp;\\leq \\tinv 2 \\norm{\\bigg}{ (\\bs U \\bs U^* - \\bs I_n) \\bs y}{2}^2 + \\tinv 2 \\max_{i \\in \\upto{r}} \\bigg| 1- \\eta \\big( \\frac{\\sigma_i}{\\sigma_{\\rm max}} \\big)^2 \\bigg|^{2(k+1)} \\norm{\\big}{ \\bs U^* \\bs y }{2}^2. \\end{split} \\end{align}\\]  with \\(\\alpha \\triangleq \\frac{\\eta}{\\norm{}{\\bs A}{}^2}\\). The third line is obtained from the second line by using the identity \\(\\norm{}{\\bs{UM}}{2}^2=\\scp{}{\\bs U \\bs M}{\\bs U \\bs M} = \\scp{}{\\bs U^* \\bs U \\bs M}{\\bs M} = \\scp{}{\\bs M}{\\bs M}=\\norm{}{\\bs M}{2}^2\\). By convention \\(\\sigma_{\\rm max} = \\sigma_1\\). In \\eqref{eq:loss}, the first error term is independent of \\(k\\). One cannot do better than targetting the fastest convergence to zero of the second term.  For a fixed stepsize \\(\\eta\\), the best choice must satisfy   \\[\\begin{align}     \\min_\\eta \\max_{i \\in \\upto{r} } \\bigg| 1- \\eta \\big( \\frac{\\sigma_i}{\\sigma_{\\rm max}} \\big)^2 \\bigg| &amp;= \\min_\\eta \\max \\bigg( \\max_{i \\in \\upto{r} } 1- \\eta \\big( \\frac{\\sigma_i}{\\sigma_{\\rm max}} \\big)^2, \\max_{i \\in \\upto{r} } \\eta \\big( \\frac{\\sigma_i}{\\sigma_{\\rm max}} \\big)^2 -1 \\bigg) \\\\     &amp;= \\min_\\eta \\max \\bigg( 1- \\eta \\big( \\frac{\\sigma_r}{\\sigma_{\\rm max}} \\big)^2,\\eta -1 \\bigg) \\end{align}\\]  which is reached when   \\[\\begin{align} \\tag{2} \\label{eq:eta} \\begin{split}     &amp;1 - \\eta \\big( \\frac{\\sigma_r}{\\sigma_{\\rm max}} \\big)^2 = \\eta - 1 \\\\     \\Leftrightarrow ~&amp; \\eta = \\frac{2}{1+(\\sigma_r/\\sigma_1)^2}. \\end{split} \\end{align}\\]  In particular, injecting \\(\\eqref{eq:eta}\\) into \\(\\eqref{eq:loss}\\) yields   \\[\\begin{align}     \\cl L_k (\\bs x^{(k)}, \\bs y) &amp;\\leq \\tinv 2 \\norm{\\bigg}{ (\\bs U \\bs U^* - \\bs I_n) \\bs y}{2}^2 + \\tinv 2 \\big| \\eta - 1 \\big|^{2(k+1)} \\norm{\\big}{ \\bs U^* \\bs y }{2}^2 \\\\     &amp;= \\tinv 2 \\norm{\\bigg}{ (\\bs U \\bs U^* - \\bs I_n) \\bs y}{2}^2 + \\tinv 2 \\bigg| \\frac{1-(\\sigma_r/\\sigma_1)^2 }{1+(\\sigma_r/\\sigma_1)^2} \\bigg|^{2(k+1)} \\norm{\\big}{ \\bs U^* \\bs y }{2}^2, \\end{align}\\]  meaning that the decreasing of the loss with the iterations of the gradient descent is slower when \\(\\sigma_r \\ll \\sigma_1\\).           Figure 1: Gradient Descent iterations as a function of the stepsize for the system  \\(\\bs A \\bs x = \\bs y\\) with \\(\\bs A \\in \\Rbb^{30\\times 30}\\) of rank \\(r=5\\).  In this experiment, the ideal choice following \\(\\eqref{eq:eta}\\) was \\(\\eta^\\star = 1.26\\).  ","categories": [],
        "tags": [],
        "url": "/GD_stepsize/",
        "teaser": null
      }]
