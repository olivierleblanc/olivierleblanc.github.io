<div class="figure-div">
    <figure>
        
        <img src="/assets/images/posts/big-data-file-formats/header.jpg" alt="similarity-header" />
        
        
            <figcaption>Image Source: <a href="https://www.pexels.com/" target="_blank">Pexels</a></figcaption>
        
    </figure>
</div>

<h2 id="introduction">Introduction</h2>

<p>As we all know, juggling data from one system 
to another, transforming it through data pipelines, storing, and later 
performing analytics can easily become expensive and inefficient as the data 
grows. We often think about scalability and how to deal with the 4Vs<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> of 
data, making sure that our system can handle dynamic flows and doesn’t get 
overwhelmed with the amount of data.</p>

<p>There are many aspects of Big Data systems that come into considerations when we 
talk about scalability. Not surprisingly, one of them is how we define the 
storage strategy. A huge bottleneck for many applications is the time it takes 
to find relevant data, processes it, and write it back to another location. As 
the data grows, managing large datasets and evolving schemas becomes a huge 
issue.</p>

<p>Whether we are running Big Data analytics on on-prem clusters with dedicated or 
bare metal servers, or on cloud infrastructure, one thing is certain: the way 
we store our data and which file format we use will have an immense impact. How 
we store the data in our datalake or warehouse is critical.</p>

<p>Among many things, choosing an appropriate file format can:</p>
<ul>
  <li>Increase read/write times</li>
  <li>Split files</li>
  <li>Support schema evolution</li>
  <li>Support compression</li>
</ul>

<p>In this article we will cover some of the most common big data formats, 
their structure, when to use them, and what benefits they can have.</p>

<p><br /></p>

<h2 id="avro">AVRO</h2>

<p>As we all know, to transfer the data or store it, we need to serialize it 
first. <a href="https://avro.apache.org/docs/current/" target="_blank">Avro</a>
is one of the systems that implements data serialization. It’s a 
row-based remote procedure call and data serialization framework. It was 
developed by Doug Cutting<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> and managed within Apache’s Hadoop project.</p>

<p>It uses JSON for defining data types and serializes data in a compact binary 
format. Additionally, Avro is a language-neutral data serialization system which 
means that theoretically any language could use Avro.</p>

<h3 id="structure">Structure</h3>

<p>An Avro file consists of:</p>
<ul>
  <li>File header and</li>
  <li>One or more file data blocks</li>
</ul>

<div class="figure-div">
    <figure>
        
        <img src="/assets/images/posts/big-data-file-formats/avro_format.png" alt="similarity-header" />
        
        
            <figcaption>Depiction of an Avro file - <a href="https://www.oreilly.com/library/view/operationalizing-the-data/9781492049517/ch04.html" target="_blank">Image Source</a></figcaption>
        
    </figure>
</div>

<p>Header contains information about the schema and codec, while blocks are 
divided into:</p>
<ul>
  <li>Counts of objects in a block</li>
  <li>Size of the serialized objects</li>
  <li>Objects themselves</li>
</ul>

<blockquote>
  <p>If you are interested, take a look at 
<a href="https://avro.apache.org/docs/current/api/java/index.html" target="_blank">Java implementation</a></p>
</blockquote>

<h3 id="schema">Schema</h3>

<p>Schema can be defined as:</p>
<ul>
  <li>JSON string,</li>
  <li>JSON object, of the form: <code class="language-plaintext highlighter-rouge">{"type": "typeName" ...attributes...}</code></li>
  <li>JSON array</li>
</ul>

<blockquote>
  <p>or Avro IDL for human readable schema</p>
</blockquote>

<p><code class="language-plaintext highlighter-rouge">typeName</code> specifies a type name which can be a primitive or complex type.</p>

<p>Primitive types are: <code class="language-plaintext highlighter-rouge">null</code>, <code class="language-plaintext highlighter-rouge">boolean</code>, <code class="language-plaintext highlighter-rouge">int</code>, <code class="language-plaintext highlighter-rouge">long</code>, <code class="language-plaintext highlighter-rouge">float</code>, <code class="language-plaintext highlighter-rouge">double</code>, 
<code class="language-plaintext highlighter-rouge">bytes</code>, and <code class="language-plaintext highlighter-rouge">string</code>.</p>

<p>There are 6 complex types: <code class="language-plaintext highlighter-rouge">record</code>, <code class="language-plaintext highlighter-rouge">enum</code>, <code class="language-plaintext highlighter-rouge">array</code>, 
<code class="language-plaintext highlighter-rouge">map</code>, <code class="language-plaintext highlighter-rouge">union</code>, and <code class="language-plaintext highlighter-rouge">fixed</code>. Each of them supports <code class="language-plaintext highlighter-rouge">attributes</code>. In case of the 
<code class="language-plaintext highlighter-rouge">record</code> type, they are:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">name</code>: name of the record (required)</li>
  <li><code class="language-plaintext highlighter-rouge">namespace</code>: string that qualifies the name</li>
  <li><code class="language-plaintext highlighter-rouge">doc</code>: documentation of the schema (optional)</li>
  <li><code class="language-plaintext highlighter-rouge">aliases</code>: alternative names of the schema (optional)</li>
  <li><code class="language-plaintext highlighter-rouge">fields</code>: list of fields, where each field is a JSON object</li>
</ul>

<blockquote>
  <p>The <code class="language-plaintext highlighter-rouge">attributes</code> of other complex types can be found in the 
<a href="https://avro.apache.org/docs/current/spec.html" target="_blank">Apache Avro Specification</a></p>
</blockquote>

<p>Now that we understand the structure of the schema, we can define it as, for 
example:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w"> </span><span class="p">{</span><span class="w">
   </span><span class="nl">"namespace"</span><span class="p">:</span><span class="w"> </span><span class="s2">"example.vladsiv"</span><span class="p">,</span><span class="w">
   </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"record"</span><span class="p">,</span><span class="w">
   </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"vladsiv"</span><span class="p">,</span><span class="w">
   </span><span class="nl">"doc"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Just a test schema"</span><span class="p">,</span><span class="w">
   </span><span class="nl">"fields"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="p">{</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"name"</span><span class="p">,</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="p">},</span><span class="w">
      </span><span class="p">{</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"year"</span><span class="p">,</span><span class="w">  </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"null"</span><span class="p">,</span><span class="w"> </span><span class="s2">"int"</span><span class="p">]}</span><span class="w">
   </span><span class="p">]</span><span class="w"> 
 </span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h3 id="why-avro">Why Avro?</h3>

<ul>
  <li>It is a very fast serialization/deserialization format, great for storing raw 
data</li>
  <li>Schema is included in the file header, which allows downstream systems to 
easily retrieve the schema (no need for external metastores)</li>
  <li>Supports evolutionary schemas - any source schema change is easily handled</li>
  <li>It’s more optimized for reading series of entire rows - since it’s row-based</li>
  <li>Good option for landing zones where we store the data for further processing 
(which usually means that we read whole files)</li>
  <li>Great integration with Kafka</li>
  <li>Supports file splitting</li>
</ul>

<p><br /></p>

<h2 id="parquet">Parquet</h2>

<p>Apache Parquet is a free and open-source column-oriented data storage format 
and it began as a joint effort between Twitter and Cloudera. It’s designed 
for efficient data storage and retrieval.</p>

<p>Running queries on the Parquet-based file-system is extremely efficient since 
the column-oriented data allows you to focus on the relevant data very quickly. 
This means that the amount of data scanned is way smaller which results in 
efficient I/O usage.</p>

<h3 id="structure-1">Structure</h3>

<p>Parquet file consists of 3 parts:</p>

<ul>
  <li>Header: which has 4-byte magic number <code class="language-plaintext highlighter-rouge">PAR1</code></li>
  <li>Data Body: contains blocks of data stored in row groups</li>
  <li>Footer: where all the metadata is stored</li>
</ul>

<p>The 4-byte magic number <code class="language-plaintext highlighter-rouge">PAR1</code>, stored in the header and footer, indicates that 
the file is in parquet format.</p>

<p>Metadata includes the version of the format, schema, information about the 
columns in the data which includes: type, path, encoding, number of values etc. 
The interesting fact is that the metadata is stored in the footer, which allows 
single pass writing.</p>

<p>So how do we read a parquet file? At the end of the file we have footer length, 
so the initial seek will be performed to read the length and then, since we 
know the length, jump to the beginning of the footer metadata.</p>

<div class="figure-div">
    <figure>
        
        <img src="/assets/images/posts/big-data-file-formats/parquet_format.png" alt="similarity-header" />
        
        
            <figcaption>Depiction of a Parquet file - <a href="https://www.oreilly.com/library/view/operationalizing-the-data/9781492049517/ch04.html" target="_blank">Image Source</a></figcaption>
        
    </figure>
</div>

<p>Why is this important?</p>

<p>Since body consists of blocks and each block has a boundary, writing 
metadata at the end (when all the blocks have been written) allows us to 
store these boundaries. This gives us a huge performance when it comes to 
locating the blocks and processing them in parallel.</p>

<p>Each block is stored in the form of <em>row groups</em>. As we can see in the image 
above, row groups have multiple columns called: <em>columnar chunks</em>. These chunks 
are further divided into <em>pages</em>. The pages store values for a particular 
column and can be compressed as the values can repeat.</p>

<h3 id="schema-1">Schema</h3>

<p>Parquet supports schema evolution. For example, we can start with a simple 
schema, and then add more columns as needed. By doing this, we end up with 
multiple Parquet files with different schemas. This is not an 
issue since the schemas are mutually compatible and Parquet supports automatic 
schema merging among those files.</p>

<p>When it comes to data types, Parquet defines a set of types that is intended to 
be as minimal as possible:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">BOOLEAN</code>: 1 bit boolean</li>
  <li><code class="language-plaintext highlighter-rouge">INT32</code>: 32 bit signed ints</li>
  <li><code class="language-plaintext highlighter-rouge">INT64</code>: 64 bit signed ints</li>
  <li><code class="language-plaintext highlighter-rouge">INT96</code>: 96 bit signed ints</li>
  <li><code class="language-plaintext highlighter-rouge">FLOAT</code>: IEEE 32-bit floating point values</li>
  <li><code class="language-plaintext highlighter-rouge">DOUBLE</code>: IEEE 64-bit floating point values</li>
  <li><code class="language-plaintext highlighter-rouge">BYTE_ARRAY</code>: arbitrarily long byte arrays.</li>
</ul>

<p>These are further extended by <em>Logical Types</em>. This keeps the set of primitive 
types to a minimum and reuses parquet’s efficient encodings. These extended 
types are:</p>

<ul>
  <li>String Types: <code class="language-plaintext highlighter-rouge">STRING</code>, <code class="language-plaintext highlighter-rouge">ENUM</code>, <code class="language-plaintext highlighter-rouge">UUID</code></li>
  <li>Numerical Types: <code class="language-plaintext highlighter-rouge">Signed Integers</code>, <code class="language-plaintext highlighter-rouge">Unsigned Integers</code>, <code class="language-plaintext highlighter-rouge">DECIMAL</code></li>
  <li>Temporal Types: <code class="language-plaintext highlighter-rouge">DATE</code>, <code class="language-plaintext highlighter-rouge">TIME</code>, <code class="language-plaintext highlighter-rouge">TIMESTAMP</code>, <code class="language-plaintext highlighter-rouge">INTERVAL</code></li>
  <li>Embedded Types: <code class="language-plaintext highlighter-rouge">JSON</code>, <code class="language-plaintext highlighter-rouge">BSON</code></li>
  <li>Nested Types: <code class="language-plaintext highlighter-rouge">Lists</code>, <code class="language-plaintext highlighter-rouge">Maps</code></li>
  <li>Unknown Types: <code class="language-plaintext highlighter-rouge">UNKNOWN</code> - always null</li>
</ul>

<p>If you are interested in details regarding Logical Types and encodings, please see: 
<a href="https://github.com/apache/parquet-format/blob/2e23a1168f50e83cacbbf970259a947e430ebe3a/LogicalTypes.md" target="_blank">Parquet Logical Type Definitions</a>
and
<a href="https://github.com/apache/parquet-format/blob/2e23a1168f50e83cacbbf970259a947e430ebe3a/Encodings.md" target="_blank">Parquet encoding definitions</a>.</p>

<h3 id="why-parquet">Why Parquet?</h3>

<ul>
  <li>When we are dealing with many columns but only want to query some of them - 
Since Parquet is column-based it’s great for analytics. As the business expands 
we usually tend to increase the number of fields in our datasets but most of our 
queries just use a subset of them</li>
  <li>Great for very large amounts of data - Techniques like data skipping 
increase data throughput and performance on large datasets</li>
  <li>Low storage consumption by implementing efficient column-wise compression</li>
  <li>Free and open source - It’s language agnostic and decouples storage from 
compute services (since most of data analytic services have support for 
Parquet out of the box)</li>
  <li>Works great with serverless cloud technologies like AWS Athena, 
Amazon Redshift Spectrum, Google BigQuery, Google Dataproc etc</li>
</ul>

<p><br /></p>

<h2 id="orc">ORC</h2>

<p>Apache ORC (Optimized Row Columnar) is a free and open-source column-oriented 
data storage format. It was first announced in February 2013 by Hortonworks in 
collaboration with Facebook<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</p>

<p>ORC provides a highly efficient way to store data using block-mode compression 
based on data types. It has great reading, writing, and processing performance 
thanks to data skipping and indexing.</p>

<h3 id="structure-2">Structure</h3>

<p>ORC file format consists of:</p>
<ul>
  <li>Groups of row data called <em>stripes</em></li>
  <li>File footer</li>
  <li>Postscript</li>
</ul>

<div class="figure-div">
    <figure>
        
        <img src="/assets/images/posts/big-data-file-formats/orc_format.png" alt="similarity-header" />
        
        
            <figcaption>Depiction of an ORC file - <a href="https://docs.cloudera.com/runtime/7.2.10/hive-performance-tuning/topics/hive_maximize_storage_resources_using_orc.html" target="_blank">Image Source</a></figcaption>
        
    </figure>
</div>

<p>The process of reading an ORC file starts at the end of the file. The final 
byte of the file contains the length of the Postscript. The Postscript is 
never compressed and provides the information about the file: metadata, 
version, compression etc. Once we parse the Postscript we can get the 
compressed form of the File Footer, decompress it, and learn more about the 
stripes stored in the file.</p>

<p>The File Footer contains information about the stripes, the number of rows per 
stripe, the type schema information, and some column-level statistics: 
count, min, max, and sum.</p>

<p>Stripes contain only entire rows and are divided into three sections:</p>
<ul>
  <li>Index data: a set of indexes for the rows within the stripe</li>
  <li>Row data</li>
  <li>Stripe footer: directory of stream locations</li>
</ul>

<p>What’s important here is that both the indexes and the row data sections are 
in column-oriented format. This allows us to read the data only for the required 
columns.</p>

<p>Index data provides information about the columns stored in row data. It 
includes min and max values for each column, but also row positions which 
provide offsets that enable row-skipping within a stripe for fast reads.</p>

<blockquote>
  <p>If you are interested in more details about the ORC file format, please see: 
<a href="https://orc.apache.org/specification/ORCv1/" target="_blank">ORC Specification v1</a></p>
</blockquote>

<h3 id="schema-2">Schema</h3>

<p>Like Avro and Parquet, ORC also supports schema evolution. This allow us to 
merge schema of multiple ORC files with different but mutually compatible 
schemas.</p>

<p>ORC provides a rich set of scalar and compound types:</p>

<ul>
  <li>Integer: <code class="language-plaintext highlighter-rouge">boolean</code>, <code class="language-plaintext highlighter-rouge">tinyint</code>, <code class="language-plaintext highlighter-rouge">smallint</code>, <code class="language-plaintext highlighter-rouge">int</code>, <code class="language-plaintext highlighter-rouge">bigint</code></li>
  <li>Floating point: <code class="language-plaintext highlighter-rouge">float</code>, <code class="language-plaintext highlighter-rouge">double</code></li>
  <li>String types: <code class="language-plaintext highlighter-rouge">string</code>, <code class="language-plaintext highlighter-rouge">char</code>, <code class="language-plaintext highlighter-rouge">varchar</code></li>
  <li>Binary blobs: <code class="language-plaintext highlighter-rouge">binary</code></li>
  <li>Date/time: <code class="language-plaintext highlighter-rouge">timestamp</code>, <code class="language-plaintext highlighter-rouge">timestamp</code> with local time zone, <code class="language-plaintext highlighter-rouge">date</code></li>
  <li>Compound types: <code class="language-plaintext highlighter-rouge">struct</code>, <code class="language-plaintext highlighter-rouge">list</code>, <code class="language-plaintext highlighter-rouge">map</code>, <code class="language-plaintext highlighter-rouge">union</code></li>
</ul>

<p>All scalar and compound types in ORC can take null values.</p>

<p>There is a nice example in the ORC documentation: 
<a href="https://orc.apache.org/docs/types.html" target="_blank">Types</a>, that 
illustrates how it works.</p>

<p>Let’s say that we have the table <code class="language-plaintext highlighter-rouge">Foobar</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>create table Foobar (
 myInt int,
 myMap map&lt;string,
 struct&lt;myString : string,
 myDouble: double&gt;&gt;,
 myTime timestamp
);
</code></pre></div></div>

<p>The columns in the file would form the following tree:</p>

<div class="figure-div">
    <figure>
        
        <img src="/assets/images/posts/big-data-file-formats/orc-schema-tree.png" alt="similarity-header" />
        
        
            <figcaption>ORC schema tree - <a href="https://orc.apache.org/docs/types.html" target="_blank">Image Source</a></figcaption>
        
    </figure>
</div>

<h3 id="why-orc">Why ORC?</h3>

<ul>
  <li>Really efficient compression - It saves a lot of storage space</li>
  <li>Support for ACID transactions - <a href="https://orc.apache.org/docs/acid.html" target="_blank">ACID Support</a></li>
  <li>Predicate Pushdown efficiency - Pushes filters into reads so that minimal 
number of columns and rows are read</li>
  <li>Schema evolution and merging</li>
  <li>Efficient and fast reads - Thanks to built-in indexes and column aggregates, 
we can skip entire stripes and focus on the data we need</li>
</ul>

<h2 id="final-words">Final Words</h2>

<p>Understanding how big data file formats work helps us make the right decision 
that will impact efficiency and scalability of our data applications. Each 
file format has its own unique internal structure and could be the right choice 
for our storage strategy, depending on the use case.</p>

<p>In this article I’ve tried to give a brief overview of some of the key points 
that will help you understand the underlying structure and benefits of popular 
big data file formats. Of course, there are many details which I didn’t cover 
and I encourage you to go through all of the referenced material to learn more.</p>

<p>I hope you enjoyed reading this article and, as always, feel free to reach 
out to me if you have any questions or suggestions.</p>

<blockquote>
  <p><strong>Bonus</strong>: AliORC (Alibaba ORC) is a deeply optimized file format based on the 
open-source Apache ORC. It is fully compatible with open-source ORC, extended 
with additional features, and optimized for Async Prefetch, I/O mode management, 
and adaptive dictionary encoding. Ref: 
<a href="https://www.alibabacloud.com/blog/aliorc-a-combination-of-maxcompute-and-apache-orc_595359" target="_blank">AliORC: A Combination of MaxCompute and Apache ORC</a></p>
</blockquote>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://www.clairvoyant.ai/blog/big-data-file-formats" target="_blank">Big Data File Formats</a></li>
  <li><a href="https://db-blog.web.cern.ch/blog/zbigniew-baranowski/2017-01-performance-comparison-different-file-formats-and-storage-engines" target="_blank">Performance comparison of different file formats and storage engines in the Hadoop ecosystem </a></li>
  <li><a href="https://databricks.com/glossary/what-is-parquet" target="_blank">Databricks - Parquet</a></li>
  <li><a href="https://www.linkedin.com/pulse/all-you-need-know-parquet-file-structure-depth-rohan-karanjawala" target="_blank">All You Need To Know About Parquet File Structure In Depth </a></li>
  <li><a href="https://cwiki.apache.org/confluence/display/hive/languagemanual+orc" target="_blank">Apache Hive - ORC</a></li>
</ul>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Volume, Variety, Velocity, and Veracity <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>The father of Hadoop and Lucene. Ref: <a href="https://en.wikipedia.org/wiki/Doug_Cutting" target="_blank">Wikipedia - Doug Cutting</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>A month later, the Apache Parquet format was announced. Ref: <a href="https://en.wikipedia.org/wiki/Apache_ORC" target="_blank">Wikipedia - Apache ORC</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
